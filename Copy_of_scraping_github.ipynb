{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of scraping_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPt3ZJEpaFVkuLFBZBnFMcE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eslam-Alaa-ZX/Scraping_github/blob/main/Copy_of_scraping_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "DqrL0UJXIyCb",
        "outputId": "6610aeab-32b1-4563-846f-7b0035d2a091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter GitHub username: Eslam-Alaa-ZX\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-75de93fc1a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mopenWebsite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-75de93fc1a17>\u001b[0m in \u001b[0;36mopenWebsite\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# get the sorted list of all repos and print top 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msorted_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mrepo_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0musername\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iteritems'"
          ]
        }
      ],
      "source": [
        "import urllib.request, urllib.parse, urllib.error\n",
        "import http.cookiejar as cookielib\n",
        "import requests\n",
        "from lxml import html\n",
        "from lxml import etree\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import operator\n",
        "\n",
        "top_limit = 9\n",
        "\n",
        "def openWebsite():\n",
        "\tusername = str(input(\"enter GitHub username: \"))\n",
        "\trepo_dict = {}\n",
        "\turl = \"https://github.com/\"+username+\"?tab=repositories\"\n",
        "\twhile True:\n",
        "\t\t# print \"url: \",url\n",
        "\t\tcj = cookielib.CookieJar()\n",
        "\t\topener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
        "\t\tresp = opener.open(url)\n",
        "\t\tdoc = html.fromstring(resp.read())\n",
        "\t\t# print html.tostring(doc, pretty_print=True)\n",
        "\t\trepo_name = doc.xpath('//li[@class=\"col-12 d-block width-full py-4 border-bottom public source\"]/div[@class=\"d-inline-block mb-1\"]/h3/a/text()')\n",
        "\n",
        "\t\trepo_list = []\n",
        "\t\t\n",
        "\t\tfor name in repo_name:\n",
        "\t\t\tname = ' '.join(''.join(name).split())\n",
        "\t\t\trepo_list.append(name)\n",
        "\t\t\trepo_dict[name] = 0\n",
        "\n",
        "\t\t# print repo_list\n",
        "\t\tresponse = requests.get(url)\n",
        "\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
        "\t\tdiv = soup.find_all('li', {'class': 'col-12 d-block width-full py-4 border-bottom public source'})\n",
        "\t\t# print div\n",
        "\t\t# \\/[a-zA-Z0-9\\-\\_\\.]+\\/[a-zA-Z0-9\\.\\-\\_]+\\/stargazers\n",
        "\t\tfor d in div:\n",
        "\t\t\ttemp = d.find_all('div',{'class':'f6 text-gray mt-2'})\n",
        "\t\t\tfor t in temp:\n",
        "\t\t\t\tx = t.find_all('a', attrs={'href': re.compile(\"^\\/[a-zA-Z0-9\\-\\_\\.]+\\/[a-zA-Z0-9\\.\\-\\_]+\\/stargazers\")})\n",
        "\t\t\t\tif len(x) is not 0:\n",
        "\t\t\t\t\tname = x[0].get('href')\n",
        "\t\t\t\t\tname = name[len(username)+2:-11]\n",
        "\t\t\t\t\trepo_dict[name] = int(x[0].text)\n",
        "\t\t\n",
        "\n",
        "\t\t# check if next page exists for more repos\n",
        "\t\tdiv = soup.find('a',{'class':'next_page'})\n",
        "\t\t# print div\n",
        "\t\tif div is not None:\n",
        "\t\t\turl = div.get('href')\n",
        "\t\t\turl = \"https://github.com/\"+url\n",
        "\t\telse:\n",
        "\t\t\tbreak\t\n",
        "\n",
        "\t# get the sorted list of all repos and print top 10\n",
        "\ti = 0\n",
        "\tsorted_repo = sorted(repo_dict.iteritems(), key = operator.itemgetter(1))\n",
        "\tfor val in reversed(sorted_repo):\n",
        "\t\trepo_url = \"https://github.com/\" + username + \"/\" + val[0]\n",
        "\t\tprint (\"\\nrepo name : \",val[0], \"\\nrepo url  : \",repo_url, \"\\nstars     : \",val[1])\n",
        "\t\ti = i + 1\n",
        "\t\tif i > top_limit:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\t\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\topenWebsite()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python3 script to fetch top 10 starred\n",
        "# repositories of a user on github\n",
        "import urllib.request, urllib.parse, urllib.error\n",
        "from urllib.parse import urlparse\n",
        "import http.cookiejar\n",
        "import requests\n",
        "from lxml import html\n",
        "from lxml import etree\n",
        "#import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import operator\n",
        "\n",
        "top_limit = 9\n",
        " \n",
        "def openWebsite():\n",
        " \n",
        "    # enter Github username\n",
        "    # of user\n",
        "    username = str(input(\"enter GitHub username: \"))\n",
        " \n",
        "    # Dictionary to store key as repository\n",
        "    # name and value as no. of stars\n",
        "    repo_dict = []\n",
        "    stars = []\n",
        " \n",
        "    # This is first page url where user\n",
        "    # repositories are located\n",
        "    url = \"https://github.com/\"+username+\"?tab=repositories\"\n",
        "    id=\"repo-stars-counter-star\"\n",
        "    web_URL = requests.get(url)\n",
        "\n",
        "\n",
        "    #save page URL\n",
        "    result = web_URL.content\n",
        " \n",
        "    # loop for all the pages\n",
        "    \n",
        " \n",
        "        # open the website and get\n",
        "        # the html of webpage into doc\n",
        "    soup = BeautifulSoup(result,\"lxml\")\n",
        "    names = soup.find_all(\"a\",{\"itemprop\":\"name codeRepository\"})\n",
        "    star={}\n",
        "    for i in range(len(names)):\n",
        "        repo_dict.append(names[i].text)\n",
        "        \n",
        "        tt=names[i].text\n",
        "        url2 = f'https://github.com/{username}/{tt}'\n",
        "        print(url2)\n",
        "        \n",
        "        web_URL2 = requests.get(url2)\n",
        "        result2 = web_URL2.content\n",
        "\n",
        "        soup2 = BeautifulSoup(result2,\"lxml\")\n",
        "        star = soup2.find_all(\"span\",{\"id\":\"repo-stars-counter-star\"})\n",
        "\n",
        "    print(star)\n",
        "    \n",
        "    for r in range(len(star)):\n",
        "      stars.append(star[r].text)\n",
        "\n",
        "   \n",
        " \n",
        "                \n",
        "        \n",
        "    for e in repo_dict:\n",
        "      print(e)\n",
        "    for y in stars:\n",
        "      print(y)\n",
        "       \n",
        " \n",
        " \n",
        "# Driver program\n",
        "if __name__ == \"__main__\":\n",
        "    openWebsite()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v5UYoZcNFnU",
        "outputId": "ab8f5d4f-6043-442b-d64c-7d30727e48d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter GitHub username: netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "https://github.com/netcode\n",
            "[]\n",
            "\n",
            "        express4-bootstrap-starter\n",
            "\n",
            "        smokescreen\n",
            "\n",
            "        eslam.github.io\n",
            "\n",
            "        security-labs-pocs\n",
            "\n",
            "        netcode.github.io\n",
            "\n",
            "        ejs\n",
            "\n",
            "        express\n",
            "\n",
            "        Spring4shell-CVE-2022-22965-POC\n",
            "\n",
            "        Spring-cloud-function-SpEL-RCE\n",
            "\n",
            "        money-tracker\n",
            "\n",
            "        stratus-red-team\n",
            "\n",
            "        PayloadsAllTheThings\n",
            "\n",
            "        integrations-core\n",
            "\n",
            "        log4shell-vulnerable-app\n",
            "\n",
            "        CVE-2021-41773_42013\n",
            "\n",
            "        CVE-2021-41773\n",
            "\n",
            "        CVE-2021-26084_PoC\n",
            "\n",
            "        awesome-golang-security\n",
            "\n",
            "        apm-agent-nodejs\n",
            "\n",
            "        nosql-injection-sequelize\n",
            "\n",
            "        content\n",
            "\n",
            "        thegoodhacker.github.io\n",
            "\n",
            "        exploits\n",
            "\n",
            "        datadog-agent\n",
            "\n",
            "        ipaddr.js\n",
            "\n",
            "        quick-nodejs-starter\n",
            "\n",
            "        better-errors\n",
            "\n",
            "        Ghost\n",
            "\n",
            "        Ghost-Admin\n",
            "\n",
            "        dvna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from requests.api import get\n",
        "import urllib.request, urllib.parse, urllib.error\n",
        "from urllib.parse import urlparse\n",
        "import http.cookiejar\n",
        "import requests\n",
        "from lxml import html\n",
        "from lxml import etree\n",
        "import csv\n",
        "from itertools import zip_longest\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import operator\n",
        "\n",
        "\n",
        "\n",
        "def openWebsite():\n",
        "\n",
        "    username = str(input(\"enter GitHub username: \"))\n",
        "    repo_info = []\n",
        "    stars = []\n",
        "    names = []\n",
        "    urls = []\n",
        "    stars = []\n",
        "    forks=[]\n",
        "    issues=[]\n",
        "\n",
        "    while True:\n",
        "\n",
        "        url = \"https://github.com/\" + username + \"?tab=repositories\"\n",
        "        web_url = requests.get(url)\n",
        "        result = web_url.content\n",
        "        soup = BeautifulSoup(result, \"lxml\")\n",
        "        listOfRipo = soup.find_all(\"ul\", {\"data-filterable-for\": \"your-repos-filter\"})\n",
        "        for repo in listOfRipo:\n",
        "            a_repo = repo.find_all(\"li\", {\n",
        "                \"class\": \"col-12 d-flex width-full py-4 border-bottom color-border-muted public source\"})+repo.find_all(\"li\", {\"class\": \"col-12 d-flex width-full py-4 border-bottom color-border-muted public fork\"})\n",
        "            \n",
        "            #temp=repo.find_all(\"li\", {\"class\": \"col-12 d-flex width-full py-4 border-bottom color-border-muted public fork\"})\n",
        "            #a_repo.append()\n",
        "            for repoN in a_repo:\n",
        "                name = repoN.find_all(\"h3\", {\"class\": \"wb-break-all\"})\n",
        "                for link in name:\n",
        "                  l=link.find_all(\"a\",{\"itemprop\":\"name codeRepository\"})\n",
        "                  for r in l:\n",
        "                      names.append(r.text)\n",
        "                      url2 = str(r.get(\"href\"))\n",
        "                      urls.append(\"https://github.com/\" +url2)\n",
        "                      \n",
        "                      web_url = requests.get(\"https://github.com/\" +url2)\n",
        "                      result = web_url.content\n",
        "                      soup = BeautifulSoup(result, \"lxml\")\n",
        "                      sta = soup.find_all(\"span\", {\"id\": \"repo-stars-counter-star\"})\n",
        "                      for rs in range(len(sta)):\n",
        "                          stars.append(sta[rs].text)\n",
        "\n",
        "                      fok = soup.find_all(\"span\", {\"id\": \"repo-network-counter\"})\n",
        "                      for fo in range(len(fok)):\n",
        "                          forks.append(fok[fo].text)\n",
        "                      \n",
        "                      iss = soup.find(\"span\", {\"id\": \"issues-repo-tab-count\"})\n",
        "                      \n",
        "                      if iss is not None:\n",
        "                          issues.append(iss.text)\n",
        "                      else:\n",
        "                          issues.append('0')\n",
        "\n",
        "        repo_info = [names, urls, stars,forks,issues]\n",
        "        export_file = zip_longest(*repo_info)\n",
        "        with open(\"repos.csv\",\"w\") as myfile:\n",
        "          wr = csv.writer(myfile)\n",
        "          wr.writerow([\"Repositorie Name\",\"Repositorie URL\",\"Number Of Stars\",\"Number Of Forks\",\"Number Of Issues\"])\n",
        "          wr.writerows(export_file)\n",
        "        \n",
        "        div = soup.find('a', {'rel': 'n'})\n",
        "        \n",
        "        if div is not None:\n",
        "            url = div.get('href')\n",
        "            url = \"https://github.com/\" + url\n",
        "        else:\n",
        "            # if there is no next repository\n",
        "            # page, then exit loop\n",
        "            break\n",
        "\n",
        "    for y in repo_info:\n",
        "      print(y);\n",
        "\n",
        "# Driver program\n",
        "if __name__ == \"__main__\":\n",
        "    openWebsite()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxXtjTmwwVtW",
        "outputId": "5754076b-7c0b-4443-8fc6-8c0d5722eab4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter GitHub username: EslaMx7\n",
            "['\\n        PasteIntoFile', '\\n        ScreenTask', '\\n        AESxWin', '\\n        sparkpost-csharp-webhooks-sample', '\\n        Simple-Streamer', '\\n        eslamx7.github.io', '\\n        ScreenTask2', '\\n        GIS-Tasks', '\\n        til', '\\n        Machine-Learning-Tasks', '\\n        Android-HTTPRemoteControl', '\\n        AI-Tasks-JADE-Tests', '\\n        ContentAddr.Azure', '\\n        wireguard-install', '\\n        rust-template', '\\n        tinyfilemanager', '\\n        createPatterns', '\\n        gitlab-explorer', '\\n        Best-README-Template', '\\n        CSS-Keylogging', '\\n        AspNetAuthorizationWorkshop', '\\n        anyproxy', '\\n        AtumyAndroidSDK', '\\n        transfer.sh', '\\n        WindowsContextMenuCustomizations', '\\n        computer-science', '\\n        AlternativesToOnlineServices', '\\n        mysql-postgresql-converter', '\\n        LambdaCrypt', '\\n        screencasts']\n",
            "['https://github.com//EslaMx7/PasteIntoFile', 'https://github.com//EslaMx7/ScreenTask', 'https://github.com//EslaMx7/AESxWin', 'https://github.com//EslaMx7/sparkpost-csharp-webhooks-sample', 'https://github.com//EslaMx7/Simple-Streamer', 'https://github.com//EslaMx7/eslamx7.github.io', 'https://github.com//EslaMx7/ScreenTask2', 'https://github.com//EslaMx7/GIS-Tasks', 'https://github.com//EslaMx7/til', 'https://github.com//EslaMx7/Machine-Learning-Tasks', 'https://github.com//EslaMx7/Android-HTTPRemoteControl', 'https://github.com//EslaMx7/AI-Tasks-JADE-Tests', 'https://github.com//EslaMx7/ContentAddr.Azure', 'https://github.com//EslaMx7/wireguard-install', 'https://github.com//EslaMx7/rust-template', 'https://github.com//EslaMx7/tinyfilemanager', 'https://github.com//EslaMx7/createPatterns', 'https://github.com//EslaMx7/gitlab-explorer', 'https://github.com//EslaMx7/Best-README-Template', 'https://github.com//EslaMx7/CSS-Keylogging', 'https://github.com//EslaMx7/AspNetAuthorizationWorkshop', 'https://github.com//EslaMx7/anyproxy', 'https://github.com//EslaMx7/AtumyAndroidSDK', 'https://github.com//EslaMx7/transfer.sh', 'https://github.com//EslaMx7/WindowsContextMenuCustomizations', 'https://github.com//EslaMx7/computer-science', 'https://github.com//EslaMx7/AlternativesToOnlineServices', 'https://github.com//EslaMx7/mysql-postgresql-converter', 'https://github.com//EslaMx7/LambdaCrypt', 'https://github.com//EslaMx7/screencasts']\n",
            "['348', '573', '59', '0', '29', '1', '2', '3', '1', '12', '17', '2', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '4', '0', '1', '0', '0', '0']\n",
            "['47', '215', '29', '0', '4', '1', '2', '0', '0', '10', '16', '7', '2', '700', '15', '1.2k', '1', '5', '14.4k', '442', '236', '1.2k', '1', '1.3k', '19', '15.6k', '13', '460', '18', '2.1k']\n",
            "['21', '23', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
          ]
        }
      ]
    }
  ]
}